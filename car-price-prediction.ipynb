{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5af7c7d6-fdb8-47fc-a3a7-495bd8f3cefd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Science Project: car price prediction\n",
    "\n",
    "The following project consists in one scenario in which you will have to analyze and train a model for one data set. The data set contains information about year, price, transmission, mileage, fuel type and engine size of used cars. The idea is training a model to predict which will be the price of a used car in the market.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1.\tUpload the csv file to the workspace and load it into a data frame.\n",
    "2.\tLook for null values and outliers. Remove, keep or impute them and explain why.\n",
    "3.\tShow the main statistics (mean, standard deviationâ€¦) of the numerical columns of the data set. Are any of the variables skewed?\n",
    "4.\tTrain a model for the prediction of the price. Explain why I chose the model that I have trained.\n",
    "5.\tTest the model and obtain some performance metrics from it. Does it have a good performance? Why?\n",
    "6.\tWould you say that you have enough information to predict the price of an Electric Vehicle of the same class? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports\n",
    "These are the needed imports for the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8ee2460-2c31-4e31-b307-2445b8aa4295",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\IKER~1.DEL\\AppData\\Local\\Temp\\32/ipykernel_43536/2314643051.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import typing\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "db417042-a8eb-4f68-a3cf-4eb8fc0cc1c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.\tUpload the csv file to the workspace and load it into a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from a local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = pd.read_csv('cclass.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f25c8b7-598d-4f3e-b793-89e4baef5713",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Check the types of data just in case the dataframe has automatically given a type for a column that might not be OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "53186ae9-4917-4a3b-aadd-e9e8c67aa9d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ae9acd10-1c85-40ff-bea2-5fd385a2c9c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The data types are correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4743b86c-8137-4323-8fd3-7864235a43c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.\tLook for null values and outliers. Remove, keep or impute them and explain why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d33cceff-a804-4a97-ad55-5ac36083792b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1. Null values\n",
    "As Machine Learning models cannot handle null values, they need to be handled before training any model. Hence, the dataset needs to be preprocessed. \n",
    "\n",
    "In general, there are 4 main strategies to handle null values:\n",
    "  1. *Drop Columns with Missing Values*: in case there is a null value or more in a column, the entire column is removed from the dataset\n",
    "  2. *Drop Rows with Missing Values*: in case there is a null value or more in a row, the entire row is removed from the dataset\n",
    "  3. *Imputation*: using a class such as SimpleImputer, predict the null values so that the dataset shape (rows and columns) is the same and the null values are predicted considering the rest of the dataset.\n",
    "  4. *Extended Imputation*: the same as the previous Imputation, but a new column is added to state that the measuremnt or row has been modified. When doing this, the model will consider also the column for the training process\n",
    "\n",
    "First, I will check which columns have null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de14e270-3567-4479-a08d-75254a348eed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get names of columns with null values\n",
    "columns_with_null_values = [col for col in df.columns\n",
    "                     if df[col].isnull().any()]\n",
    "print('These columns have null values:', columns_with_null_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f3d4786-a500-4370-8e27-09bc099e0521",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I would like to display the null values before performing any further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14f56553-fbee-4cd8-b483-a98e2d2f5d20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "null_data = df[df.isnull().any(axis=1)]\n",
    "print(null_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f1bc497b-d1e2-46ad-9deb-91a981a24ce2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1.0 Setting up the scenario for the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e0733b7e-a123-4001-a512-b6b9ef4caea1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this steps, I will set up a scenario for making a comparison of the 4 mentioned strategies for handling null values.\n",
    "\n",
    "This reference scenario will have the next characteristics:\n",
    "- **Random Forest Model**: for this regression problem it is good enough for making a benchmark for the null value handling strategies.\n",
    "- **Mean Absolute Error (MAE)**: metric widely used for regression problems. I am using this one since it shows on average how of are the predictions. Hence, the smallest the MAE, the better result.\n",
    "\n",
    "I will also perform some changes in the dataset so that it can be introduced in the model.\n",
    "\n",
    "First, I will start removing the **model** column since it is the same for all the rows so it does not provide any value to the model to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0f36685-d714-4971-a1e3-6b1b4659a8c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Unique values for the model:\", df['model'].unique())\n",
    "df_without_model_column = df.drop('model', axis=1)\n",
    "df_without_model_column.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "311cd318-d677-45d5-a20a-ec674dcddd19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As in the models numerical data is only allowed, I will have to transform the categorical columns **transmission** and **fuelType**. \n",
    "\n",
    "I will use One-Hot encoding since there is no order in these two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e91edf87-bdef-40ce-9e7a-e5ad2f5f0dbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get list of categorical variables\n",
    "s = (df_without_model_column.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\", object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb83cdeb-dea6-4418-af68-666c3f954dce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols = pd.DataFrame(OH_encoder.fit_transform(df_without_model_column[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols.index = df_without_model_column.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "numerical_df: pd.DataFrame = df_without_model_column.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "encoded_df: pd.DataFrame = pd.concat([numerical_df, OH_cols], axis=1)\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d3af3e74-353f-4560-a041-ca915376c5eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As the dataset is ready, now I will create a function to process the train-test split and obtain the MAE. For the train-test split 80% of the data will be used for training and 20% for testing, which is a regular practice in machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c99ddef6-3d0f-46de-8eb7-c1e5fb0bda66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function for comparing different approaches\n",
    "def score_dataset(df:pd.DataFrame, target_name:str, train_size:float=0.8, test_size:float=0.2, n_estimators:int=10)->float:\n",
    "    target_column = target_name\n",
    "    target = df[target_column]\n",
    "    predictors = df.drop([target_column], axis=1)\n",
    "  \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(predictors,\n",
    "                                                          target,\n",
    "                                                          train_size=train_size,\n",
    "                                                          test_size=test_size,\n",
    "                                                          random_state=0)\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_valid)\n",
    "  \n",
    "    return mean_absolute_error(y_valid, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "77f3a69d-c4a7-4c9e-9d9d-59e46d457baa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1.1 Drop Columns with Missing Values\n",
    "\n",
    "This is the code for dropping columns that contain missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d8d1e713-ac85-4230-aff3-bea7f96f1830",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing:List = [col for col in encoded_df.columns\n",
    "                     if encoded_df[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "drop_columns_df:pd.DataFrame = encoded_df.drop(cols_with_missing, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "abfdcff0-a4fb-4387-8def-c6d5052dd899",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's get the score for this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e175934e-e172-471d-bb6a-de846b1f5b8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAE_drop_columns = score_dataset(df=drop_columns_df, target_name='price')\n",
    "print(\"MAE when dropping columns that contain missing values:\", MAE_drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29d52f5b-be15-4a6c-b6ce-736f4975f750",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1.2 Drop Rows with Missing Values\n",
    "\n",
    "This is the code for dropping rows that contain missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec1f516e-eb76-412e-91b8-497c509cc9ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the rows with missing values\n",
    "drop_rows_df:pd.DataFrame = encoded_df.dropna(axis=0) \n",
    "\n",
    "# Get the score\n",
    "MAE_drop_rows = score_dataset(df=drop_rows_df, target_name='price')\n",
    "print(\"MAE when dropping rows that contain missing values:\", MAE_drop_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "618a147d-e3bb-426e-9970-d755e4b3e1b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1.3 Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c37ab34c-906d-45a1-a176-dff8d530db28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Impute the missing values using the most frequent value per column as the columns that have missing values data are categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f25624ce-85a5-47fb-a18e-51f54c8c7cf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imputation\n",
    "my_imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputed_df = pd.DataFrame(my_imputer.fit_transform(encoded_df))\n",
    "\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_df.columns = encoded_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "436b88ad-9aaa-4e3c-a1a9-2a8c8e2d2693",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Get the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0467729a-348c-4f9d-a6dc-60a6cd915890",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAE_imputation = score_dataset(df=imputed_df, target_name='price')\n",
    "print(\"MAE when imputing missing values:\", MAE_imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c08669cd-c653-4301-b670-808d269eb7a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1.4 Extended Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "25c690ca-c103-499e-8d8c-510200144968",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It is similar to the previous imputation, but extra columns will be added to state if the measurement (row) has been imputed. In this way, the model will have more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7287bea1-f941-4350-897b-9ba8ade56acc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make copy to avoid changing original data (when imputing)\n",
    "encoded_df_plus = encoded_df.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    encoded_df_plus[col + '_was_missing'] = encoded_df_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputed_df_plus = pd.DataFrame(my_imputer.fit_transform(encoded_df_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_df_plus.columns = encoded_df_plus.columns\n",
    "imputed_df_plus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c3d4e6d-70af-4c8a-a216-2a41b792536d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAE_extended_imputation = score_dataset(df=imputed_df_plus, target_name='price')\n",
    "print(\"MAE when performing extended imputation for missing values:\", MAE_extended_imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "594f58a5-0d3d-4b12-808b-90caaf722572",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1.5 Summary for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3812bb47-e3db-47ba-9fe5-90d963c4e7d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's print again the MAEs for each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "477bac8d-32fd-413b-9d26-a48d9dd2ba3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"MAE when dropping columns that contain missing values:\", MAE_drop_columns)\n",
    "print(\"MAE when dropping rows that contain missing values:\", MAE_drop_rows)\n",
    "print(\"MAE when imputing missing values:\", MAE_imputation)\n",
    "print(\"MAE when performing extended imputation for missing values:\", MAE_extended_imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf008b1a-c05e-4b64-8977-3a7ebb4d6291",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The best strategy for handling missing values for this particular dataset is the **Imputation** since it obtains the smallest MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "67c4664c-32b6-4e25-bdfd-cce3e5acd3f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.2. Outliers\n",
    "\n",
    "Outliers will be only considered for numerical variables: **year**, **price**, **mileage**, and **engineSize**.\n",
    "\n",
    "To understand better what to do with outliers, their distributions will be obtained first. The dataframe to be used will be the one from before the missing values section that had removed the **model** column.\n",
    "\n",
    "I will perform the analysis by variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d72da57e-e10b-4e8b-a97b-65940e248522",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2.1 year column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2cee52d2-b277-4691-97a3-b8efedbddf05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's make a historgram for the **year** column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4991bb07-3614-401d-83f7-8c12ab167eac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_without_model_column.year.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f313f446-4d7b-4d87-8315-4850c4314a62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Despite the distribution is left skewed, it looks like there are no outliers in this category. Just to see if the years are OK, I will check the minium and maximums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "014a8a3c-7203-4ead-b4b1-b0abf7cc4dfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Min year:\", df_without_model_column.year.min())\n",
    "print(\"Max year:\", df_without_model_column.year.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers seem reasonable as they make reference to years of the C-class model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3716cf2c-c2ed-4609-b81d-e8e069d0bb13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2.2 price column\n",
    "\n",
    "Again, I will perform a histogram to have a look to the data distribution for the **price** column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1d0f4acb-4b12-4214-8a5a-c4448759d336",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_without_model_column.price.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8932d828-39ff-4667-a3ea-d35209ac23d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the case of the **price**, it looks like the distribution is right-skewed. Therefore, for outlier detection the Inter-Quartile Range (IQR) proximity rule can be used. Before that, let's make a box-plot for more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9fbf4c94-506c-4112-bf3b-e1632aa582a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y=df_without_model_column.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "13f37208-da27-4b68-b901-1447b54fa092",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From the box-plot it is possible to see that there are outliers on the upper side but not in the lower side. Nevertheless, the next function, which is based in the Inter Quartile Range, will be helpfull for finding both types of outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e19cb887-42c4-4316-b13c-9baf8fd01df0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def lower_upper_iqr(df , column):\n",
    "    global lower,upper\n",
    "    q25, q75 = np.nanquantile(df[column], 0.25), np.nanquantile(df[column], 0.75)\n",
    "    # calculate the IQR\n",
    "    iqr = q75 - q25\n",
    "    # calculate the outlier cutoff\n",
    "    cut_off = iqr * 1.5\n",
    "    # calculate the lower and upper bound value\n",
    "    lower_iqr_limit = q25 - cut_off\n",
    "    upper_iqr_limit = q75 + cut_off\n",
    "    \n",
    "    return lower_iqr_limit, upper_iqr_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11863c7a-f2de-4dbf-8489-5b9a5d7a1ac4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With the function defined, I will obtain the Inter Quartile limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "352fa3f4-0918-45a6-a5a6-2c03cf3466e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lower_iqr_limit, upper_iqr_limit = lower_upper_iqr(df_without_model_column, 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b88a402-85f7-4e1d-b614-4d3ac88ece3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For better understanding, the next plot will show the outliers in red color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bcb76763-6cf6-410c-83c9-a5260361baa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "sns.distplot(df_without_model_column.price, kde=False)\n",
    "plt.axvspan(xmin = lower_iqr_limit,xmax= df_without_model_column.price.min(),alpha=0.2, color='red')\n",
    "plt.axvspan(xmin = upper_iqr_limit,xmax= df_without_model_column.price.max(),alpha=0.2, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04c82e34-c8a1-4cba-a5e5-39206885fbe6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.2.0 Setting up the scenario for the benchmark of outliers\n",
    "\n",
    "As it happened with the missing values, for the outliers there are two main strategies that can be performed with outliers:\n",
    "1. *Drop the outlier rows*: remove the rows that contain outliers\n",
    "2. *Impute the outliers*: replace the outlier value with the mean of the distribution\n",
    "\n",
    "For the evaluation of these two scenarios the *score_dataset* function will be reused. Besides, two other functions will be made from the previous section of missing values:\n",
    "\n",
    "- *One-Hot-Encoding*: to convert the categorical variables into numerical\n",
    "- *Imputation*: as imputation was getting the lowest MAE in the missing value section, this strategy will be used\n",
    "\n",
    "Finally, a function that performs those two steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4561b1b-4656-4f63-b1b5-b0eeb1421459",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Get list of categorical variables\n",
    "    s = (df.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "\n",
    "    # Apply one-hot encoder to each column with categorical data\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols = pd.DataFrame(OH_encoder.fit_transform(df[object_cols]))\n",
    "\n",
    "    # One-hot encoding removed index; put it back\n",
    "    OH_cols.index = df.index\n",
    "\n",
    "    # Remove categorical columns (will replace with one-hot encoding)\n",
    "    numerical_df: pd.DataFrame = df.drop(object_cols, axis=1)\n",
    "\n",
    "    # Add one-hot encoded columns to numerical features\n",
    "    one_hot_encoded_df: pd.DataFrame = pd.concat([numerical_df, OH_cols], axis=1)\n",
    "    \n",
    "    return one_hot_encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e5a9f767-6db7-447c-9b23-69d61c4679e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def imputation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Imputation\n",
    "    my_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    imputed_df = pd.DataFrame(my_imputer.fit_transform(df))\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_df.columns = df.columns\n",
    "  \n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0fb6978b-e09d-46af-882e-afca1c6defdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def score_dataset_outliers(df:pd.DataFrame, target_name:str, train_size:float=0.8, test_size:float=0.2, n_estimators:int=10)->float:\n",
    "    one_hot_encoding_df = one_hot_encoding(df)\n",
    "    imputed_df = imputation(one_hot_encoding_df)\n",
    "    mae = score_dataset(df=imputed_df, target_name=target_name, train_size=train_size, test_size=test_size, n_estimators=n_estimators)\n",
    "    \n",
    "    return mae  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dbb6b1c5-94e6-4568-9c63-18446b0aed50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.2.1 Drop the outlier rows\n",
    "\n",
    "Once the outliers have been identified, their rows will be removed from the dataset in this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3644625-ced3-4b54-92eb-7a251043da33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the data frame\n",
    "drop_price_outliers_df: pd.DataFrame = df_without_model_column.copy()\n",
    "# Remove the outliers\n",
    "drop_price_outliers_df = drop_price_outliers_df[(drop_price_outliers_df['price'] > lower_iqr_limit) & (drop_price_outliers_df['price'] < upper_iqr_limit)]\n",
    "\n",
    "print(\"Shape of the original dataframe:\", df_without_model_column.shape)\n",
    "print(\"Shape of the dataframe after dropping price outliers:\", drop_price_outliers_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "74f77a52-420e-409a-84fc-89d32c90ea0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Get the MAE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63e72e6b-9321-4107-95e8-2f0498f6ce53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAE_drop_price_outliers = score_dataset_outliers(df=drop_price_outliers_df, target_name='price')\n",
    "print(\"MAE when dropping outliers for the price column:\", MAE_drop_price_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af468fa9-8aae-4939-a579-d323e0004645",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.2.2 Impute the outliers\n",
    "\n",
    "In this strategy the identified outliers will be imputed using the median value of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f786eae1-c614-4d6f-847e-ef58101db366",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the data frame\n",
    "impute_price_outliers_df: pd.DataFrame = df_without_model_column.copy()\n",
    "#Imputation\n",
    "median = df_without_model_column['price'].median()\n",
    "impute_price_outliers_df['price'] = np.where(df_without_model_column['price'] > upper_iqr_limit, median, df_without_model_column['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the MAE score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8dab651-7dad-45c3-b181-f0f607c8013b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAE_impute_price_outliers = score_dataset_outliers(df=impute_price_outliers_df, target_name='price')\n",
    "print(\"MAE when imputing outliers for the price column:\", MAE_impute_price_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3227e4e3-0792-4fb7-b764-20df4bf3d4e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.2.3 Summary\n",
    "\n",
    "This is the summary for both outlier strategies for the **price** column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8cc130df-b543-4f58-9589-59061963b622",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"MAE when dropping outliers for the price column:\", MAE_drop_price_outliers)\n",
    "print(\"MAE when imputing outliers for the price column:\", MAE_impute_price_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6b55eed9-124e-4b5a-a173-d6aa5e1f5e5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this case dropping the rows with outliers in **price** is a better solution. This is its distribution after dropping the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "110eac8b-b1e3-468a-b487-ff725b5a0d14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drop_price_outliers_df.price.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b017d663-3995-48e3-84b7-71c0db51d784",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2.3 mileage column\n",
    "\n",
    "Again, I will perform a histogram to have a look to the data distribution for the **mileage** column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5cfc2e34-c96b-40c4-b412-0f004e9d42dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drop_price_outliers_df.mileage.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86b29dbb-31fe-4076-b99f-994731f4d8a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the case of the **mileage**, it looks like the distribution is right-skewed. Therefore, for outlier detection the Inter-Quartile Range (IQR) proximity rule can be used. Before that, let's make a box-plot for more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba18fae2-55c6-4954-b552-c1dd9c107a42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y=drop_price_outliers_df.mileage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e7440993-68e0-42f6-ad0b-df5dab65ab0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From the box-plot it is possible to see that there are outliers on the upper side but not in the lower side. With the Inter Quartile Range function we will obtain the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e2e92cc1-3eaf-40ed-ba4a-0954ac7a9efe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lower_iqr_limit, upper_iqr_limit = lower_upper_iqr(drop_price_outliers_df, 'mileage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb4743a0-b872-4206-87e6-ce5730e54317",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For better understanding, the next plot will show the outliers in red color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b811101e-f552-47d9-978d-6388edb67931",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "sns.distplot(drop_price_outliers_df.mileage, kde=False)\n",
    "plt.axvspan(xmin = lower_iqr_limit,xmax= drop_price_outliers_df.mileage.min(),alpha=0.2, color='red')\n",
    "plt.axvspan(xmin = upper_iqr_limit,xmax= drop_price_outliers_df.mileage.max(),alpha=0.2, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d76b33a-6fea-4729-92d1-c1bf3a608b6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As before, the same two strategies will be checked with the MAE to know what is the best option to follow (dropping the rows that containt outliers or imput the outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7fd7ce8f-bc66-4cee-bbca-8edc2f42aabf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.3.1 Drop the outlier rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "10fdc486-17e7-4fba-abbf-00049b23d753",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the data frame\n",
    "drop_mileage_outliers_df: pd.DataFrame = drop_price_outliers_df.copy()\n",
    "# Remove the outliers\n",
    "drop_mileage_outliers_df = drop_mileage_outliers_df[(drop_mileage_outliers_df['mileage'] > lower_iqr_limit) & (drop_mileage_outliers_df['mileage'] < upper_iqr_limit)]\n",
    "\n",
    "print(\"Shape of the original dataframe:\", df_without_model_column.shape)\n",
    "print(\"Shape of the dataframe after dropping mileage outliers:\", drop_mileage_outliers_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c1b246a-406d-43f8-9410-b0c4d0c9749c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAE_drop_mileage_outliers = score_dataset_outliers(df=drop_mileage_outliers_df, target_name='price')\n",
    "print(\"MAE when dropping outliers for the mileage column:\", MAE_drop_mileage_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "91012a53-8fa0-4aa1-8386-5dff4f0c4316",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.3.2 Impute the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7f3cc3f-f3a0-4ed1-b946-c8f03181cdaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the data frame\n",
    "impute_mileage_outliers_df: pd.DataFrame = drop_price_outliers_df.copy()\n",
    "#Imputation\n",
    "median = drop_price_outliers_df['mileage'].median()\n",
    "impute_mileage_outliers_df['mileage'] = np.where(drop_price_outliers_df['mileage'] > upper_iqr_limit, median, drop_price_outliers_df['mileage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44cd8f79-1b73-4bf9-ac53-40651247bc1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MAE_impute_mileage_outliers = score_dataset_outliers(df=impute_mileage_outliers_df, target_name='price')\n",
    "print(\"MAE when imputing outliers for the mileage column:\", MAE_impute_mileage_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6d0f08a-e620-4697-a0a3-10f6d2d71ab9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.3.3 Summary\n",
    "\n",
    "These are the MAE for both strategies for mileage outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac42b699-f693-43b3-9d05-76bb1027374e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"MAE when dropping outliers for the mileage column:\", MAE_drop_mileage_outliers)\n",
    "print(\"MAE when imputing outliers for the mileage column:\", MAE_impute_mileage_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f7f0c9f0-b2e7-4949-adb0-8a3bf0333908",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this case imputing the rows with outliers in **mileage** is a better solution. This is the its distribution after imputing the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21061a52-c562-44dc-b2ed-135d707434d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "impute_mileage_outliers_df.mileage.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad366da6-def1-4217-92b5-4963092bb697",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2.4 engineSize column\n",
    "\n",
    "Again, I will perform a histogram to have a look to the data distribution for the **engineSize** column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1fa87b0e-87ba-448e-819d-b5bfa4f5f68a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "impute_mileage_outliers_df.engineSize.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df9672c2-6f5e-48b2-a649-391cf760d8ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this case the **engineSize** variable is more limited on its range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe4cbd61-de06-4051-b5fa-116473e4c591",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_engine_size = impute_mileage_outliers_df.engineSize.max()\n",
    "min_engine_size = impute_mileage_outliers_df.engineSize.min()\n",
    "print(\"Max engine size:\", max_engine_size)\n",
    "print(\"Min engine size:\", min_engine_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b06b5285-b38a-4baa-a78d-638ff1548f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Having an engien size of 0.0 looks strange. Let's check the dataframe that has that condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33d282fa-0b67-44b5-8b13-2a454da6689b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "impute_mileage_outliers_df[impute_mileage_outliers_df['engineSize'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0f56e9c4-3d94-4fdc-b1bf-5bdf818304c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is an error, since it is a Diesel engine and 0.0 engineSize is not possible. Before doing anything else, the unique values for engineSize will be also checked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2552e77-f3e8-49fd-9a9b-6d951acb4f2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_engine_size = impute_mileage_outliers_df['engineSize'].unique()\n",
    "print(\"Sorted unique engine sizes:\", sorted(set(unique_engine_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "77ab2115-7f81-4fed-a31d-66ed9ce8aaaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To verify that the **engineSize** is correct, more documentation is needed in order to see if those engine sizes match with the c-class model. Hence, I will only remove the 0.0 enginesize measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0516e062-9ea2-45af-8b13-3fc3873a8b6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drop_engine_size_outliers_df = impute_mileage_outliers_df[impute_mileage_outliers_df['engineSize'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06e1927f-af52-4d73-9fee-8ced722a2cea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.\tMain statistics\n",
    "\n",
    "For this section I will be using the original dataset. These are the main statistics per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6a77b08f-b15c-4c5a-85c6-6038ff1f463d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a987e596-68b4-4033-b022-bf28ef97089f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To know if the variables are skewed, it is neccesary to plot a histogram per numerical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8af35716-e52b-4d0b-bc6c-d374c1bad638",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.1\tyear\n",
    "\n",
    "The year variable is left-skewed as it can be seen on the next plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70d041a6-55c9-4f13-a017-5ec547db1db1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.year.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e4c6e6b3-839e-408b-b542-f1eb0925d183",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.2\tprice\n",
    "\n",
    "The price variable is right-skewed as it can be seen on the next plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "780e101c-fc68-446c-a2cd-12e15abd2d5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.price.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "01fe6bf1-c758-4ef0-9845-dd4367b3ac4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.3 mileage\n",
    "\n",
    "The mileage variable is right-skewed as it can be seen on the next plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06c0c7b4-3d6b-4fdb-ba4c-3c4186aa7433",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.mileage.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da73a25f-8870-4e45-b180-4a67fee5bd6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.4\tengineSize\n",
    "\n",
    "It can be said that the engineSize variable is slightly right-skewed despite it has a different distribution as the engineSize has some specific values (it is not a real continouos variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "87929964-e516-4795-a43a-9371edc85bab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.engineSize.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0deea042-ec0e-4eac-ab59-ccd751a6629d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.\tTrain and test a model\n",
    "\n",
    "In this section I will perform section 4 and 5 since I think that training and testing go together in a Data Science project. Thanks to the testing, it is possible to check how the training went. Hence, having them together makes more sense to me.\n",
    "\n",
    "The dataset that will be used is the last one from the outliers section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8324dddb-bc6e-4a4e-961c-8603e3f02066",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_df = drop_engine_size_outliers_df\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e81859f0-651c-480b-806b-7a81b16d1fac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I will show the shape of the original dataset and the preprocessed, to see how it has changed. Remember that the **model** column has been removed and that some rows also were removed when handling outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81aca369-1299-4b34-8e53-3ac1b2edc78b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Original dataset shape:\", df.shape)\n",
    "print(\"Preprocessed dataset shape:\", preprocessed_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac9cf3df-8c12-4125-9c9b-2c44133ef5df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As mentioned before during the missing value section, **imputation** will be performed as this strategy obtained the lowest Mean Absolute Error (MAE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "320650ec-7513-463e-b51c-04f88708f361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imputed_df = imputation(df=preprocessed_df)\n",
    "imputed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1b4e38c5-26d7-4d36-adff-e7ff89531c37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I will check again the data types of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a26b8b2a-2323-42d1-b423-614728b02301",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imputed_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "07ffc6e9-b767-4738-ad45-35a157ffd761",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The types need to be adjusted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_correct_types_df = imputed_df.astype({'year': 'int', 'price': 'float', 'mileage': 'float', 'engineSize': 'float'}, copy=False)\n",
    "imputed_correct_types_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is possible to perform one hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = one_hot_encoding(imputed_correct_types_df)\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is ready for starting the training-test phase. \n",
    "\n",
    "Before, I have used train-test split. However, better results can be obtained using cross-validation. The computation effort is greater with cross-validation, but as this dataset is not so big, it makes sense to use cross-validation.\n",
    "\n",
    "In the following cells I will check if the cross-validation is better than the train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for comparing different approaches\n",
    "def cross_validation_mae(df:pd.DataFrame, target_name:str, model)->float:\n",
    "    target_column = target_name\n",
    "    target = df[target_column]\n",
    "    predictors = df.drop([target_column], axis=1)\n",
    "\n",
    "    scores = cross_val_score(model, predictors, target, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    return -1 * scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain the MAE for a Random Forest model that uses cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "MAE_cross_validation = cross_validation_mae(df=encoded_df, target_name='price', model=random_forest)\n",
    "print(\"MAE for cross validation with Random Forest:\", MAE_cross_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare, let's try the same Random Forest model but with train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_train_test_split = score_dataset(df=encoded_df, target_name='price')\n",
    "print(\"MAE for train-test split with Random Forest:\", MAE_train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-test split** is better according to the obtained MAE since it is smaller, so I will use train-test split from now on.\n",
    "\n",
    "Now, I will make some experiments with different models that usually are good for regression problems like this one. These will be the models I will be using:\n",
    "- Lasso\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "I will redesign the score_dataset function so that different models can be easily added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset_by_model(df:pd.DataFrame, target_name:str, model, train_size:float=0.8, test_size:float=0.2)->float:\n",
    "    target_column = target_name\n",
    "    target = df[target_column]\n",
    "    predictors = df.drop([target_column], axis=1)\n",
    "  \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(predictors,\n",
    "                                                          target,\n",
    "                                                          train_size=train_size,\n",
    "                                                          test_size=test_size,\n",
    "                                                          random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_valid)\n",
    "  \n",
    "    return mean_absolute_error(y_valid, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha=0.1)\n",
    "lasso_mae = score_dataset_by_model(df=encoded_df, target_name='price', model=lasso)\n",
    "print(\"MAE with Lasso:\", lasso_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = tree.DecisionTreeRegressor()\n",
    "decision_tree_mae = score_dataset_by_model(df=encoded_df, target_name='price', model=decision_tree)\n",
    "print(\"MAE with Decision Tree:\", decision_tree_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor()\n",
    "random_forest_mae = score_dataset_by_model(df=encoded_df, target_name='price', model=random_forest)\n",
    "print(\"MAE with Decision Tree:\", random_forest_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = XGBRegressor()\n",
    "xgboost_model_mae = score_dataset_by_model(df=encoded_df, target_name='price', model=xgboost_model)\n",
    "print(\"MAE with XGBoost:\", xgboost_model_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Model summary\n",
    "\n",
    "These are the MAEs for different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE with Lasso:\", lasso_mae)\n",
    "print(\"MAE with Decision Tree:\", decision_tree_mae)\n",
    "print(\"MAE with Decision Tree:\", random_forest_mae)\n",
    "print(\"MAE with XGBoost:\", xgboost_model_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the lowest Mean Absolute Error (MAE) is obtained by the **XGBoost model**, this one will be used to obtain more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Would you say that you have enough information to predict the price of an Electric Vehicle of the same class? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset does not contain any Electric Vehicle model, it will be difficult to make a price prediction due to different factors compared with the regular car:\n",
    "- Materials\n",
    "- Production processes\n",
    "- Maintenance\n",
    "- Durability\n",
    "- Type of clients\n",
    "- Others\n",
    "\n",
    "It is true that in the dataset there is a **fuelType** that is *Other* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fuelType.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That *Other* might be considered as electric, but there are still other technologies such as gas and hydrogen engines that might pollute the results. Therefore, I think that it is not safe to make a price prediction of an Electric Vehicle according to this dataset. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "data_science_aptitude_test",
   "notebookOrigID": 262524176034455,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
